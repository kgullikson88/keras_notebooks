{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (normal neural network) on the Reuters newswire classification\n",
    "\n",
    "The original script that this notebook is based on is [here](https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Settings\n",
    "\n",
    "- **`max_words`**: Only keep this many words as features. Uses the most common words.\n",
    "- Iterations:  These values set the number of iterations.\n",
    "    - **`batch_size`**: The number of samples per gradient update. Bigger values make the gradient update more accurate, but mean it takes longer to train the neural network\n",
    "    - **`nb_epoch`**: The number of times to go through all of the training data. Since `batch_size` is less than the full training set size, each \"epoch\" will be updating the gradient multiple times. So basically, the number of iterations is `nb_epoch * sample_size / batch_size`.    \n",
    "- **`nb_hidden`**: The number of hidden layers to use\n",
    "- **`nb_dense`**: The number of units to use in the hidden layer(s).\n",
    "- **`p_dropout`**: Randomly sets this fraction of the input units to 0 at each gradient update. It helps to prevent overfitting.\n",
    "\n",
    "## Network Architecture:\n",
    "\n",
    "Here is something close to what the neural network we use here looks like.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/400px-Artificial_neural_network.svg.png\">\n",
    "\n",
    "Each of the input nodes correspond to a yes/no answer to the questions \"Does this article contain the word 'x'?\" In our model, we have `max_words` input nodes instead of the 3 shown here.\n",
    "\n",
    "The next layer, the hidden layer, is where a lot of the magic happens. Each hidden layer node input is a linear combination of the input layer values. Their output is a nonlinear \"activation\" function applied to the input. Typical activation functions are `tanh` or in this case, [`relu`](https://en.wikipedia.org/wiki/Rectifier_(neural_networks). The more hidden layer nodes you have, the more accurate the neural network can be.\n",
    "\n",
    "The output layer in our case is the number of types of news articles. Like the hidden layer, each node is a linear combination of the previous layer's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "batch_size = 32\n",
    "nb_epoch = 15\n",
    "nb_dense = 512\n",
    "nb_hidden = 1   # The number of hidden layers to use\n",
    "p_dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "X_train shape: (8982, 1000)\n",
      "X_test shape: (2246, 1000)\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "Y_train shape: (8982, 46)\n",
      "Y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(nb_words=max_words, test_split=0.2)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "nb_classes = np.max(y_train)+1\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(nb_words=max_words)\n",
    "X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(nb_dense, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(p_dropout))\n",
    "for _ in range(nb_hidden-1):\n",
    "    model.add(Dense(nb_dense))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(p_dropout))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/15\n",
      "8083/8083 [==============================] - 1s - loss: 1.4274 - acc: 0.6812 - val_loss: 1.0934 - val_acc: 0.7553\n",
      "Epoch 2/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.7737 - acc: 0.8169 - val_loss: 0.9172 - val_acc: 0.7920\n",
      "Epoch 3/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.5441 - acc: 0.8691 - val_loss: 0.8529 - val_acc: 0.8087\n",
      "Epoch 4/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.4132 - acc: 0.8968 - val_loss: 0.8730 - val_acc: 0.8076\n",
      "Epoch 5/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.3338 - acc: 0.9180 - val_loss: 0.8931 - val_acc: 0.8176\n",
      "Epoch 6/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.2751 - acc: 0.9287 - val_loss: 0.9250 - val_acc: 0.8176\n",
      "Epoch 7/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.2381 - acc: 0.9380 - val_loss: 0.9519 - val_acc: 0.8109\n",
      "Epoch 8/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.2198 - acc: 0.9440 - val_loss: 0.9600 - val_acc: 0.8098\n",
      "Epoch 9/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.2000 - acc: 0.9478 - val_loss: 1.0384 - val_acc: 0.7964\n",
      "Epoch 10/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.1850 - acc: 0.9490 - val_loss: 1.0403 - val_acc: 0.7864\n",
      "Epoch 11/15\n",
      "8083/8083 [==============================] - 1s - loss: 0.1829 - acc: 0.9522 - val_loss: 1.0429 - val_acc: 0.7898\n",
      "Epoch 12/15\n",
      "8083/8083 [==============================] - 2s - loss: 0.1706 - acc: 0.9536 - val_loss: 1.1037 - val_acc: 0.7898\n",
      "Epoch 13/15\n",
      "8083/8083 [==============================] - 2s - loss: 0.1684 - acc: 0.9545 - val_loss: 1.0831 - val_acc: 0.7931\n",
      "Epoch 14/15\n",
      "8083/8083 [==============================] - 2s - loss: 0.1581 - acc: 0.9553 - val_loss: 1.1036 - val_acc: 0.7887\n",
      "Epoch 15/15\n",
      "8083/8083 [==============================] - 2s - loss: 0.1562 - acc: 0.9535 - val_loss: 1.1101 - val_acc: 0.7998\n",
      "Model training took 0.48 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    nb_epoch=nb_epoch, batch_size=batch_size,\n",
    "                    verbose=1, validation_split=0.1)\n",
    "t2 = time.time()\n",
    "print('Model training took {:.2g} minutes'.format((t2-t1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016/2246 [=========================>....] - ETA: 0s\n",
      "Test score: 1.08631595775\n",
      "Test accuracy: 0.786286731968\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Save fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_model.save_model(model, 'Reuters_MLP_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
